# Copyright (c) MONAI Consortium
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#     http://www.apache.org/licenses/LICENSE-2.0
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations
import torch.nn as nn
import torch 
from functools import partial

from mamba_ssm import Mamba
import torch.nn.functional as F 

class LayerNorm(nn.Module):
    r""" LayerNorm that supports two data formats: channels_last (default) or channels_first.
    The ordering of the dimensions in the inputs. channels_last corresponds to inputs with
    shape (batch_size, height, width, channels) while channels_first corresponds to inputs
    with shape (batch_size, channels, height, width).
    """
    def __init__(self, normalized_shape, eps=1e-6, data_format="channels_last"):
        super().__init__()
        self.weight = nn.Parameter(torch.ones(normalized_shape))
        self.bias = nn.Parameter(torch.zeros(normalized_shape))
        self.eps = eps
        self.data_format = data_format
        if self.data_format not in ["channels_last", "channels_first"]:
            raise NotImplementedError
        self.normalized_shape = (normalized_shape, )

    def forward(self, x):
        if self.data_format == "channels_last":
            return F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)
        elif self.data_format == "channels_first":
            u = x.mean(1, keepdim=True) #torch.Size([3, 4, 1, 768])
            s = (x - u).pow(1).mean(1, keepdim=True) #torch.Size([3, 4, 100, 768])
            x = (x - u) / torch.sqrt(s + self.eps)
            x = self.weight[None, :, None, None] * x + self.bias[None, :, None, None]

            return x

class MambaLayer(nn.Module):
    def __init__(self, dim, d_state = 16, d_conv = 4, expand = 2):
        super().__init__()
        self.dim = dim
        self.norm = nn.LayerNorm(dim)
        self.mamba = Mamba(
                d_model=dim, # Model dimension d_model
                d_state=d_state,  # SSM state expansion factor
                d_conv=d_conv,    # Local convolution width
                expand=expand,    # Block expansion factor
                bimamba_type="v2",
        )
    
    def forward(self, x):
        B, C = x.shape[:2]
        assert C == self.dim
        n_tokens = x.shape[2:].numel()
        img_dims = x.shape[2:]
        x_flat = x.reshape(B, C, n_tokens).transpose(-1, -2)
        x_norm = self.norm(x_flat)
        x_mamba = self.mamba(x_norm)
        out = x_mamba.transpose(-1, -2).reshape(B, C, *img_dims)
        return out
    
class MlpChannel(nn.Module):
    def __init__(self,hidden_size, mlp_dim, ):
        super().__init__()
        self.fc1 = nn.Conv1d(hidden_size, mlp_dim, 1)
        self.act = nn.GELU()
        self.fc2 = nn.Conv1d(mlp_dim, hidden_size, 1)

    def forward(self, x): #torch.Size([4, 8, 40, 768])
        x = self.fc1(x)
        x = self.act(x)
        x = self.fc2(x)
        return x

class MLP(nn.Module):
    """ Very simple multi-layer perceptron (also called FFN)"""

    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):
        super().__init__()
        self.num_layers = num_layers
        h = [hidden_dim] * (num_layers - 1)
        self.layers = nn.ModuleList(nn.Linear(n, k) for n, k in zip([input_dim] + h, h + [output_dim]))

    def forward(self, x):
        for i, layer in enumerate(self.layers):
            x = F.relu(layer(x)) if i < self.num_layers - 1 else layer(x)
        return x
    
class MambaEncoder(nn.Module):
    def __init__(self, batch, in_chans, time, length):
        super().__init__()

        self.downsample_layer = nn.LayerNorm(length)
        # self.downsample_layer = nn.Sequential(
        #       LayerNorm(in_chans, eps=1e-6, data_format="channels_first")
        #       )
        self.stage = nn.Sequential(
            MambaLayer(dim=in_chans)
        )
        self.norm_layer = nn.LayerNorm(length)
        # self.norm_layer = nn.Sequential(
        #       LayerNorm(in_chans, eps=1e-6, data_format="channels_first")
        #       )
        
        self.mlp = MLP(256, 1024, 256, 3)
        # self.mlp = MlpChannel(in_chans, 4 * in_chans)

    def forward_features(self, x): #torch.Size([4, 8, 40, 768, 100])

        x = self.downsample_layer(x)
        x = self.stage(x) 

        x_out = self.norm_layer(x) #torch.Size([12, 100, 3, 256])
        out = self.mlp(x_out) 
        
        return out

    def forward(self, x):
        x = self.forward_features(x)
        return x


# #layer * batch, channels, time, length
# batch, channels, time, length = 12, 100, 3, 256
# x = torch.randn(batch, channels, time, length).to("cuda")
# model = MambaEncoder(batch, channels, time, length).to("cuda")
# y = model(x) 
# assert y.shape == x.shape
